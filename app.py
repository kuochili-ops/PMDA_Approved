
import streamlit as st
import pandas as pd
import requests
import io
import re
import time

# è®€å– Streamlit Cloud Secrets Manager çš„é‡‘é‘°
AZURE_KEY = st.secrets["AZURE_KEY"]
AZURE_REGION = st.secrets["AZURE_REGION"]

# Microsoft Translator API è¨­å®š
endpoint = "https://api.cognitive.microsofttranslator.com/translate"
params = {"api-version": "3.0", "from": "ja", "to": ["zh-Hant", "en"]}
headers = {
    "Ocp-Apim-Subscription-Key": AZURE_KEY,
    "Ocp-Apim-Subscription-Region": AZURE_REGION,
    "Content-type": "application/json"
}

# KEGG æŸ¥è©¢ç‰‡å‡åä¸»æˆåˆ†è‹±æ–‡å­¸å
def kegg_drug_english_name(katakana_name):
    url = f"https://rest.kegg.jp/find/drug/{katakana_name}"
    try:
        resp = requests.get(url, timeout=10)
        if resp.ok and resp.text:
            line = resp.text.split('\n')[0]
            fields = line.split()
            if len(fields) > 1:
                names = fields[1].split(';')
                # ç¯©é¸è‹±æ–‡åç¨±ï¼ˆå…¨è‹±æ–‡ä¸”é¦–å­—æ¯å¤§å¯«ï¼‰
                english_names = [n.strip() for n in names if n.strip().encode('utf-8').isalpha() and n.strip()[0].isupper()]
                if english_names:
                    return english_names[0]
                return names
    except Exception as e:
        return f"æŸ¥è©¢å¤±æ•—: {e}"
    return "æŸ¥ç„¡æ¨™æº–å­¸å"

# Microsoft Translator ç¿»è­¯ï¼ˆå•†å“åã€åŠŸæ•ˆç­‰ï¼‰
def translate_drug_info_ms(japanese_data_list):
    results = []
    for item in japanese_data_list:
        body = [{"text": f"{item['trade_name_jp']} {item['ingredient_jp']} {item['efficacy_jp']}"}]
        response = requests.post(endpoint, params=params, headers=headers, json=body)
        data = response.json()[0]["translations"]
        results.append({
            "trade_name_zh": data[0]["text"],
            "trade_name_en": data[1]["text"],
            "ingredient_zh": data[0]["text"],
            "ingredient_en": data[1]["text"],
            "efficacy_zh": data[0]["text"],
            "efficacy_en": data[1]["text"]
        })
    return results

# è™•ç†ä¸Šå‚³æª”æ¡ˆ
def process_uploaded_file(uploaded_file):
    try:
        filename = uploaded_file.name
        file_type = uploaded_file.type
        filename_lower = filename.lower()
        if 'excel' in file_type or filename_lower.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(uploaded_file, sheet_name=0, skiprows=2)
        elif 'csv' in file_type or filename_lower.endswith('.csv'):
            csv_data = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
            df = pd.read_csv(csv_data, skiprows=2)
        else:
            st.error("ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ã€‚")
            return None
        df.columns = df.columns.str.replace(r'[\s\n\u3000]', '', regex=True)
        rename_map = {}
        for col in df.columns:
            if re.match(r'^è²©.*å£².*å.*', col):
                rename_map[col] = 'Trade_Name_JP'
            elif re.match(r'^æˆ.*åˆ†.*å.*', col):
                rename_map[col] = 'Ingredient_JP'
            elif re.match(r'^åŠ¹èƒ½.*åŠ¹æœ.*', col):
                rename_map[col] = 'Efficacy_JP'
            elif col == 'æ‰¿èªæ—¥':
                rename_map[col] = 'Approval_Date'
            elif col == 'åˆ†é‡':
                rename_map[col] = 'Category'
            elif col.startswith('No'):
                rename_map[col] = 'No'
            elif col.startswith('æ‰¿èª'):
                rename_map[col] = 'Approval_Type'
        df = df.rename(columns=rename_map)
        key_cols = ['Category', 'Approval_Date', 'No', 'Trade_Name_JP', 'Approval_Type', 'Ingredient_JP', 'Efficacy_JP']
        df = df[key_cols].dropna(subset=['Trade_Name_JP', 'Ingredient_JP', 'Efficacy_JP'], how='all').reset_index(drop=True)
        return df
    except Exception as e:
        st.error(f"è™•ç†æª”æ¡ˆ {uploaded_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

# æ•´åˆç¿»è­¯èˆ‡ KEGG æŸ¥è©¢
def translate_and_combine(df):
    # 1. ä¸»æˆåˆ†æŸ¥ KEGG
    st.info("æ­£åœ¨æŸ¥è©¢ä¸»æˆåˆ†è‹±æ–‡å­¸åï¼ˆKEGG APIï¼‰...")
    ingredient_en_list = []
    for idx, row in df.iterrows():
        jp_name = row['Ingredient_JP']
        en_name = kegg_drug_english_name(jp_name)
        ingredient_en_list.append(en_name)
        time.sleep(0.34)  # æ§åˆ¶é€Ÿç‡ï¼Œé¿å…è¢« KEGG å°é–

    # 2. å…¶ä»–æ¬„ä½ç”¨ Microsoft Translator
    data_for_translation = df.apply(
        lambda row: {
            'trade_name_jp': row['Trade_Name_JP'],
            'ingredient_jp': row['Ingredient_JP'],
            'efficacy_jp': row['Efficacy_JP']
        },
        axis=1
    ).tolist()
    st.info(f"æ­£åœ¨ç¿»è­¯ {len(data_for_translation)} ç­†è—¥å“è³‡æ–™ï¼ˆå•†å“åã€åŠŸæ•ˆï¼‰...")
    translated_results = translate_drug_info_ms(data_for_translation)
    df_translated = pd.DataFrame(translated_results)

    # 3. åˆä½µçµæœ
    final_df = pd.concat([df.reset_index(drop=True), df_translated.reset_index(drop=True)], axis=1)
    final_df['Ingredient Name (English)'] = ingredient_en_list

    # 4. æ¬„ä½é¡¯ç¤ºåç¨±
    display_names = {
        'Category': 'åˆ†é‡ (Category)',
        'Approval_Date': 'æ‰¿èªæ—¥',
        'No': 'No.',
        'Trade_Name_JP': 'è²©è³£å/å…¬å¸ (æ—¥æ–‡)',
        'trade_name_zh': 'å•†å“åç¨±/å…¬å¸ (ä¸­æ–‡)',
        'trade_name_en': 'Trade Name/Company (English)',
        'Ingredient_JP': 'æˆåˆ†å (æ—¥æ–‡)',
        'ingredient_zh': 'æˆåˆ†åç¨± (ä¸­æ–‡)',
        'Ingredient Name (English)': 'Ingredient Name (English)',
        'Approval_Type': 'æ‰¿èªé¡å‹',
        'Efficacy_JP': 'åŠŸæ•ˆãƒ»æ•ˆæœ (æ—¥æ–‡)',
        'efficacy_zh': 'åŠŸæ•ˆãƒ»æ•ˆæœ (ä¸­æ–‡)',
        'efficacy_en': 'Efficacy/Effects (English)'
    }
    final_df = final_df.rename(columns=display_names)
    return final_df

# Streamlit ä¸»ç¨‹å¼
def main():
    st.set_page_config(layout="wide", page_title="PMDA æ—¥æœ¬æ–°è—¥ç¿»è­¯åˆ—è¡¨ç”Ÿæˆå™¨")
    st.title("ğŸ‡¯ğŸ‡µ PMDA æ—¥æœ¬æ–°è—¥ç¿»è­¯åˆ—è¡¨ç”Ÿæˆå™¨ (KEGG+Microsoft Translator ç‰ˆ)")
    uploaded_files = st.file_uploader(
        "ä¸Šå‚³æ–°è—¥åˆ—è¡¨æª”æ¡ˆ (CSV/XLSX)",
        type=['csv', 'xlsx', 'xls'],
        accept_multiple_files=True
    )
    if uploaded_files:
        for uploaded_file in uploaded_files:
            df = process_uploaded_file(uploaded_file)
            if df is not None:
                translated_df = translate_and_combine(df)
                if translated_df is not None:
                    st.subheader(f"ç¿»è­¯çµæœï¼š{uploaded_file.name}")
                    # åˆ†æ®µä¾æœˆä»½é¡¯ç¤º
                    translated_df["æœˆä»½"] = pd.to_datetime(translated_df["æ‰¿èªæ—¥"], errors="coerce").dt.month.astype(str) + "æœˆ"
                    month_groups = translated_df.groupby("æœˆä»½")
                    tabs = st.tabs([f"{month}" for month in month_groups.groups.keys()])
                    for i, (month, group_df) in enumerate(month_groups):
                        with tabs[i]:
                            st.header(f"{month} ç¿»è­¯çµæœ")
                            st.dataframe(group_df, use_container_width=True, hide_index=True)
                            # ä¸‹è¼‰æŒ‰éˆ•
                            csv_export = group_df.to_csv(index=False).encode('utf-8')
                            st.download_button(
                                label=f"ğŸ“¥ ä¸‹è¼‰ {month} ç¿»è­¯çµæœ (CSV)",
                                data=csv_export,
                                file_name=f"{uploaded_file.name}_{month}_Translated.csv",
                                mime='text/csv'
                            )

if __name__ == "__main__":
    main()
