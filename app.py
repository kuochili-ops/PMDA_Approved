import streamlit as st
import pandas as pd
import json
import time
import requests
import io

# --- é…ç½® (Configuration) ---
# åœ¨ Canvas ç’°å¢ƒä¸­ï¼ŒAPI Key æœƒè¢«è‡ªå‹•æä¾›ã€‚åœ¨å¤–éƒ¨ç’°å¢ƒï¼Œè«‹ç¢ºä¿æ‚¨æœ‰è¨­ç½® GEMINI_API_KEY
# For Canvas environment, leave API_KEY as empty string.
API_KEY = "" 
MODEL_NAME = "gemini-2.5-flash-preview-09-2025"
API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}"


def translate_drug_info(japanese_data_list):
    """
    ä½¿ç”¨ Gemini API ç¿»è­¯è—¥å“è³‡è¨Šåˆ—è¡¨ï¼Œä¸¦è¦æ±‚çµæ§‹åŒ–çš„ JSON è¼¸å‡ºã€‚
    """
    if not japanese_data_list:
        return []

    # é™åˆ¶ API å‘¼å«çš„è³‡æ–™é‡ï¼Œé¿å…è¶…å‡ºä¸Šä¸‹æ–‡è¦–çª—
    # For large lists, translating in batches is safer, but for typical lists, one call is efficient.
    # We will process one file (one list) at a time, which is usually safe.

    system_prompt = (
        "You are an expert pharmaceutical translator. Translate the provided Japanese drug information "
        "into Traditional Chinese and English. You MUST return a single JSON array that matches the provided JSON schema. "
        "Maintain the original Japanese text if the Japanese column contains complex formatting or identifiers. "
        "The translation must be accurate and concise."
    )

    # å°‡è¦ç¿»è­¯çš„è³‡æ–™æ ¼å¼åŒ–ç‚ºå–®ä¸€å­—ä¸²
    data_to_translate = "\n---\n".join([
        f"Trade Name (JP): {item['trade_name_jp']}\nIngredient (JP): {item['ingredient_jp']}\nEfficacy (JP): {item['efficacy_jp']}"
        for item in japanese_data_list
    ])

    user_query = f"Translate the following Japanese drug entries. Respond ONLY with the JSON array.\n\n{data_to_translate}"

    # å®šç¾©çµæ§‹åŒ– JSON è¼¸å‡ºæ ¼å¼
    response_schema = {
        "type": "ARRAY",
        "items": {
            "type": "OBJECT",
            "properties": {
                "trade_name_zh": {"type": "STRING", "description": "Traditional Chinese translation of the trade name and company."},
                "trade_name_en": {"type": "STRING", "description": "English translation of the trade name and company."},
                "ingredient_zh": {"type": "STRING", "description": "Traditional Chinese translation of the ingredient name."},
                "ingredient_en": {"type": "STRING", "description": "English translation of the ingredient name."},
                "efficacy_zh": {"type": "STRING", "description": "Traditional Chinese translation of the efficacy and effects."},
                "efficacy_en": {"type": "STRING", "description": "English translation of the efficacy and effects."}
            },
            "required": ["trade_name_zh", "trade_name_en", "ingredient_zh", "ingredient_en", "efficacy_zh", "efficacy_en"]
        }
    }
    
    payload = {
        "contents": [{"parts": [{"text": user_query}]}],
        "systemInstruction": {"parts": [{"text": system_prompt}]},
        "generationConfig": {
            "responseMimeType": "application/json",
            "responseSchema": response_schema
        }
    }

    # å¯¦ä½œæŒ‡æ•¸é€€é¿ (Exponential Backoff) è™•ç† API å‘¼å«å¤±æ•—
    max_retries = 5
    for attempt in range(max_retries):
        try:
            response = requests.post(
                API_URL,
                headers={'Content-Type': 'application/json'},
                data=json.dumps(payload),
                timeout=60 # çµ¦äºˆè¶³å¤ çš„ API åŸ·è¡Œæ™‚é–“
            )
            response.raise_for_status() # æª¢æŸ¥ HTTP ç‹€æ…‹ç¢¼
            
            result = response.json()
            
            # å¾çµæœä¸­æå– JSON å­—ä¸²
            json_text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text')
            
            if json_text:
                return json.loads(json_text)
            
            st.error("API å›æ‡‰æˆåŠŸï¼Œä½†æœªæ‰¾åˆ°é æœŸçš„ JSON ç¿»è­¯çµæœã€‚")
            return None

        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt # 1s, 2s, 4s, 8s...
                time.sleep(wait_time)
            else:
                st.error(f"ç¶“é {max_retries} æ¬¡å˜—è©¦å¾Œï¼ŒAPI å‘¼å«ä»å¤±æ•—ã€‚éŒ¯èª¤: {e}")
                return None
        except json.JSONDecodeError:
            st.error("ç¿»è­¯çµæœæ ¼å¼éŒ¯èª¤ï¼Œç„¡æ³•è§£æ JSONã€‚")
            return None
        except Exception as e:
            st.error(f"ç¿»è­¯éç¨‹ä¸­ç™¼ç”Ÿæ„å¤–éŒ¯èª¤: {e}")
            return None
            
    return None


def process_uploaded_file(uploaded_file):
    """
    è®€å– CSV æˆ– XLSX æª”æ¡ˆï¼Œæ¸…ç†è³‡æ–™ï¼Œä¸¦è­˜åˆ¥æœˆä»½åç¨±ã€‚
    """
    try:
        # 1. è­˜åˆ¥æœˆä»½åç¨±
        filename = uploaded_file.name
        # å˜—è©¦å¾æª”åä¸­æå–æœˆä»½ï¼Œä¾‹å¦‚ 'æ‰¿èªå“ç›®5æœˆåˆ†.csv' -> '5æœˆåˆ†'
        month_name_match = filename.split('æ‰¿èªå“ç›®')[-1].replace('.csv', '').replace('.xlsx - ', '').replace('.xlsx', '')
        month_name = month_name_match.strip() if month_name_match.strip() else "æœªçŸ¥æœˆä»½"
        
        # 2. è®€å–æª”æ¡ˆ
        file_type = uploaded_file.type
        filename_lower = uploaded_file.name.lower()
        
        # æ ¹æ“š PMDA æª”æ¡ˆçµæ§‹ï¼Œè·³éå‰ 2 è¡Œæ¨™é ­ (skiprows=2)
        if 'excel' in file_type or filename_lower.endswith(('.xlsx', '.xls')):
            # è®€å– Excel æª”æ¡ˆ
            # å°‡ä¸Šå‚³çš„æª”æ¡ˆç‰©ä»¶ç›´æ¥å‚³éçµ¦ read_excel
            df = pd.read_excel(uploaded_file, sheet_name=0, skiprows=2)
        elif 'csv' in file_type or filename_lower.endswith('.csv'):
            # è®€å– CSV æª”æ¡ˆ
            # å¿…é ˆä½¿ç”¨ io.StringIO è™•ç† Streamlit çš„ä¸Šå‚³ç‰©ä»¶çš„å…§å®¹
            csv_data = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
            df = pd.read_csv(csv_data, skiprows=2)
        else:
            st.error("ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ã€‚è«‹ä¸Šå‚³ CSV æˆ– XLSX æª”æ¡ˆã€‚")
            return None, None


        # 3. æ¸…ç†èˆ‡é‡å‘½åæ¬„ä½
        # é—œéµä¿®æ­£: ä½¿ç”¨æ­£å‰‡è¡¨é”å¼å»é™¤æ‰€æœ‰ç©ºæ ¼ (åŠå½¢\sã€å…¨å½¢ã€€) å’Œæ›è¡Œç¬¦è™Ÿ\nï¼Œä»¥ç¢ºä¿æ­£ç¢ºåŒ¹é…æ—¥æ–‡æ¬„ä½åç¨±ã€‚
        df.columns = df.columns.str.replace(r'[\s\nã€€]', '', regex=True)
        
        japanese_cols = {
            # ä¿®æ­£å¾Œçš„éµå (å¿…é ˆèˆ‡æ¸…ç†å¾Œçš„ DataFrame æ¬„ä½åç¨±å®Œå…¨åŒ¹é…)
            'è²©è³£å(æœƒç¤¾åã€æ³•äººç•ªè™Ÿ)': 'Trade_Name_JP',
            'æˆåˆ†å(ä¸‹ç·š:æ–°æœ‰åŠ¹æˆåˆ†)': 'Ingredient_JP',
            'åŠ¹èƒ½ãƒ»åŠ¹æœç­‰': 'Efficacy_JP',
            'æ‰¿èªæ—¥': 'Approval_Date',
            'åˆ†é‡': 'Category',
            'No.': 'No',
            'æ‰¿èª': 'Approval_Type'
        }
        
        # æª¢æŸ¥æ¬„ä½æ˜¯å¦å­˜åœ¨å¾Œæ‰é€²è¡Œé‡å‘½å
        cols_to_rename = {k: v for k, v in japanese_cols.items() if k in df.columns}
        if len(cols_to_rename) < 7: # è‡³å°‘è¦æœ‰ä¸‰å€‹ä¸»è¦æ¬„ä½
             st.error("éŒ¯èª¤: æª”æ¡ˆæ¨™é ­çµæ§‹èˆ‡é æœŸçš„ PMDA åˆ—è¡¨ä¸ç¬¦ã€‚è«‹ç¢ºèªæª”æ¡ˆå…§å®¹æ˜¯å¦æ­£ç¢ºã€‚")
             return None, None

        df = df.rename(columns=cols_to_rename)
        
        # 4. ç¯©é¸é—œéµæ¬„ä½ä¸¦æ¸…ç†ç©ºè¡Œ
        key_cols = ['Category', 'Approval_Date', 'No', 'Trade_Name_JP', 'Approval_Type', 'Ingredient_JP', 'Efficacy_JP']
        # æª¢æŸ¥é—œéµåˆ—æ˜¯å¦å…¨éƒ¨å­˜åœ¨
        missing_cols = [col for col in key_cols if col not in df.columns]
        if missing_cols:
             # å¦‚æœ missing_cols ä¸ç‚ºç©ºï¼Œè¡¨ç¤ºé‡å‘½åå¾Œä»æœ‰æ¬„ä½ç¼ºå¤±ï¼Œé€™ä¸æ‡‰è©²ç™¼ç”Ÿåœ¨æˆåŠŸçš„é‡å‘½åå¾Œï¼Œä½†ä½œç‚ºæœ€çµ‚é˜²è­·ã€‚
             st.error(f"éŒ¯èª¤: è™•ç†å¾Œçš„ DataFrame ç¼ºå°‘é—œéµæ¬„ä½: {', '.join(missing_cols)}ã€‚")
             return None, None
        
        df = df[key_cols].dropna(subset=['Trade_Name_JP', 'Ingredient_JP', 'Efficacy_JP'], how='all').reset_index(drop=True)
        
        return month_name, df

    except Exception as e:
        # é‡å°è®€å– Excel/CSV æª”æ¡ˆæœ¬èº«çš„éŒ¯èª¤é€²è¡Œå ±å‘Š
        st.error(f"è™•ç†æª”æ¡ˆ **{uploaded_file.name}** æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚è«‹ç¢ºèªæª”æ¡ˆæ˜¯æ­£ç¢ºçš„ PMDA åˆ—è¡¨æ ¼å¼ (CSV æˆ– XLSX)ã€‚éŒ¯èª¤è¨Šæ¯: {e}")
        return None, None
    
    
def translate_and_combine(df):
    """å‘¼å«ç¿»è­¯å‡½å¼ä¸¦å°‡çµæœåˆä½µå› DataFrameã€‚"""
    
    # æº–å‚™ç¿»è­¯è³‡æ–™
    data_for_translation = df.apply(
        lambda row: {
            'trade_name_jp': row['Trade_Name_JP'],
            'ingredient_jp': row['Ingredient_JP'],
            'efficacy_jp': row['Efficacy_JP']
        },
        axis=1
    ).tolist()

    st.info(f"æ­£åœ¨ç¿»è­¯ {len(data_for_translation)} ç­†è—¥å“è³‡æ–™...")
    
    translated_results = translate_drug_info(data_for_translation)
    
    if translated_results is None:
        return None
    
    # æª¢æŸ¥çµæœæ•¸é‡æ˜¯å¦åŒ¹é…
    if len(translated_results) != len(df):
        st.warning(f"ç¿»è­¯çµæœæ•¸é‡ ({len(translated_results)}) èˆ‡åŸå§‹è³‡æ–™æ•¸é‡ ({len(df)}) ä¸ç¬¦ã€‚è«‹é‡è©¦æˆ–æª¢æŸ¥åŸå§‹è³‡æ–™ã€‚")
        return None
        
    # åˆä½µè³‡æ–™
    df_translated = pd.DataFrame(translated_results)
    final_df = pd.concat([df.reset_index(drop=True), df_translated.reset_index(drop=True)], axis=1)

    # é‡æ–°æ’åºå’Œå‘½åæ¬„ä½ä»¥ä¾›é¡¯ç¤º
    final_cols = [
        'Category', 'Approval_Date', 'No', 
        'Trade_Name_JP', 'trade_name_zh', 'trade_name_en',
        'Ingredient_JP', 'ingredient_zh', 'ingredient_en',
        'Approval_Type',
        'Efficacy_JP', 'efficacy_zh', 'efficacy_en'
    ]
    
    display_names = {
        'Category': 'åˆ†é‡ (Category)',
        'Approval_Date': 'æ‰¿èªæ—¥',
        'No': 'No.',
        'Trade_Name_JP': 'è²©è³£å/å…¬å¸ (æ—¥æ–‡)',
        'trade_name_zh': 'å•†å“åç¨±/å…¬å¸ (ä¸­æ–‡)',
        'trade_name_en': 'Trade Name/Company (English)',
        'Ingredient_JP': 'æˆåˆ†å (æ—¥æ–‡)',
        'ingredient_zh': 'æˆåˆ†åç¨± (ä¸­æ–‡)',
        'ingredient_en': 'Ingredient Name (English)',
        'Approval_Type': 'æ‰¿èªé¡å‹',
        'Efficacy_JP': 'åŠŸæ•ˆãƒ»æ•ˆæœ (æ—¥æ–‡)',
        'efficacy_zh': 'åŠŸæ•ˆãƒ»æ•ˆæœ (ä¸­æ–‡)',
        'efficacy_en': 'Efficacy/Effects (English)'
    }
    
    final_df = final_df[final_cols].rename(columns=display_names)
    
    return final_df

# --- Streamlit æ‡‰ç”¨ç¨‹å¼ä¸»é«” ---

def main():
    st.set_page_config(layout="wide", page_title="PMDA æ—¥æœ¬æ–°è—¥ç¿»è­¯åˆ—è¡¨ç”Ÿæˆå™¨")
    
    st.title("ğŸ‡¯ğŸ‡µ PMDA æ—¥æœ¬æ–°è—¥ç¿»è­¯åˆ—è¡¨ç”Ÿæˆå™¨")
    st.markdown("è«‹ä¸Šå‚³å¾ [PMDA ç¶²ç«™](https://www.pmda.go.jp/review-services/drug-reviews/review-information/p-drugs/0039.html) ä¸‹è¼‰çš„æ–°è—¥æ‰¿èªå“ç›®åˆ—è¡¨æª”æ¡ˆã€‚")
    st.markdown("ç¨‹å¼å°‡è‡ªå‹•è®€å–ã€æ¸…ç†ï¼Œä¸¦ä½¿ç”¨ **Gemini API** å°‡è—¥å“è³‡è¨Šç¿»è­¯ç‚º**ä¸­æ–‡ (ç¹é«”)** åŠ **è‹±æ–‡**ã€‚")

    # åˆå§‹åŒ– Session State ä¾†å„²å­˜å·²è™•ç†çš„è³‡æ–™
    if 'processed_data' not in st.session_state:
        st.session_state.processed_data = {}

    # 1. æª”æ¡ˆä¸Šå‚³ (æ›´æ–°ä»¥æ”¯æ´ XLSX)
    uploaded_files = st.file_uploader(
        "é¸æ“‡å¤šå€‹æœˆä»½çš„æ–°è—¥åˆ—è¡¨æª”æ¡ˆ (æ”¯æ´ CSV æˆ– XLSX æ ¼å¼)",
        type=['csv', 'xlsx', 'xls'],
        accept_multiple_files=True
    )
    
    if uploaded_files:
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æ–°æª”æ¡ˆéœ€è¦è™•ç†
        files_to_process = [
            f for f in uploaded_files 
            if f.name not in st.session_state.processed_data 
            or st.session_state.processed_data[f.name].get('needs_reprocess', False)
        ]
        
        if files_to_process:
            
            # ä½¿ç”¨é€²åº¦æ¢é¡¯ç¤ºè™•ç†ç‹€æ…‹
            processing_bar = st.progress(0, text="æº–å‚™é–‹å§‹è™•ç†æª”æ¡ˆ...")
            
            for i, uploaded_file in enumerate(files_to_process):
                processing_bar.progress((i) / len(files_to_process), text=f"è™•ç†ä¸¦ç¿»è­¯ä¸­: **{uploaded_file.name}**")
                
                # æ¸…ç†æª”æ¡ˆåç¨±ä»¥ä¾›é¡¯ç¤ºå’Œå„²å­˜
                month_name, df = process_uploaded_file(uploaded_file)
                
                if df is not None:
                    # ç¿»è­¯è³‡æ–™
                    translated_df = translate_and_combine(df)
                    
                    if translated_df is not None:
                        # å„²å­˜æˆåŠŸçš„çµæœ
                        st.session_state.processed_data[uploaded_file.name] = {
                            'month_name': month_name,
                            'df': translated_df,
                            'error': False,
                            'needs_reprocess': False
                        }
                    else:
                        # å„²å­˜ç¿»è­¯å¤±æ•—çš„æ¨™è¨˜
                        st.session_state.processed_data[uploaded_file.name] = {
                            'month_name': month_name,
                            'df': None,
                            'error': True,
                            'needs_reprocess': False
                        }
                else:
                    # å„²å­˜è™•ç†å¤±æ•—çš„æ¨™è¨˜
                    st.session_state.processed_data[uploaded_file.name] = {
                        'month_name': "æœªçŸ¥æœˆä»½",
                        'df': None,
                        'error': True,
                        'needs_reprocess': False
                    }

            processing_bar.progress(1.0, text="æ‰€æœ‰æª”æ¡ˆè™•ç†å®Œç•¢ï¼")
            time.sleep(1)
            processing_bar.empty()
            st.success("æ‰€æœ‰æ–°æª”æ¡ˆè™•ç†å®Œç•¢ï¼")


        # 2. çµæœé¡¯ç¤º (ä½¿ç”¨ Tab)
        
        # éæ¿¾å‡ºæˆåŠŸè™•ç†çš„æª”æ¡ˆ
        successful_files = {k: v for k, v in st.session_state.processed_data.items() if v['df'] is not None}
        
        if successful_files:
            # å»ºç«‹åˆ†é åç¨±åˆ—è¡¨
            tab_names = [data['month_name'] for data in successful_files.values()]
            
            # å»ºç«‹åˆ†é 
            tabs = st.tabs(tab_names)
            
            # é¡¯ç¤ºæ¯å€‹åˆ†é çš„å…§å®¹
            for i, (filename, data) in enumerate(successful_files.items()):
                month_name = data['month_name']
                df = data['df']
                
                with tabs[i]:
                    st.header(f"æ–°è—¥æ‰¿èªå“ç›®åˆ—è¡¨ï¼š{month_name}")
                    st.subheader("å·²ç¿»è­¯çµæœ (ä¸­æ–‡/è‹±æ–‡)")
                    
                    # é¡¯ç¤ºå¯äº’å‹•çš„è¡¨æ ¼
                    st.dataframe(df, use_container_width=True, hide_index=True)
                    
                    # 3. ä¸‹è¼‰æŒ‰éˆ•
                    csv_export = df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label=f"ğŸ“¥ ä¸‹è¼‰ {month_name} ç¿»è­¯åˆ—è¡¨ (CSV)",
                        data=csv_export,
                        file_name=f"PMDA_Approval_List_{month_name}_Translated.csv",
                        mime='text/csv'
                    )
        
        # 4. è™•ç†å¤±æ•—æª”æ¡ˆçš„æç¤º
        failed_files = {k: v for k, v in st.session_state.processed_data.items() if v.get('error') and v['df'] is None}
        if failed_files:
            st.error("ä»¥ä¸‹æª”æ¡ˆè™•ç†æˆ–ç¿»è­¯å¤±æ•—ï¼š")
            for filename in failed_files.keys():
                st.write(f"- {filename}")
            st.markdown("è«‹ç¢ºèªæª”æ¡ˆç‚ºæ¨™æº– PMDA åˆ—è¡¨æ ¼å¼ï¼Œä¸”å…§å®¹ç¬¦åˆé æœŸçµæ§‹ã€‚")


if __name__ == "__main__":
    main()
